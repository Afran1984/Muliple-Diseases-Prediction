# -*- coding: utf-8 -*-
"""HDPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DUE2ySEWFA6PEE5NSzMAplW_1Xf23zMo
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/heart disease dataset/Heart_disease.csv')
df.head()

df.tail()

df.shape

df.info()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.shape

df.isna().sum()

df.nunique()

count_target = df['AgeCategory'].value_counts()
count_target

age = df.groupby(['AgeCategory', 'HeartDisease']), ['HeartDisease'].count
age_categories = sorted(df['AgeCategory'].unique())

fig = plt.figure(figsize=(14,4))
sns.countplot(x='AgeCategory', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"], order=age_categories)

plt.title('AgeCategory and Heart Disease')

count_target = df['Race'].value_counts()
count_target

race = df.groupby(['Race', 'HeartDisease']), ['HeartDisease'].count

fig = plt.figure(figsize=(14,4))
sns.countplot(x='Race', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('Race and Heart Disease')

count_target = df['Diabetic'].value_counts()
count_target

df['Diabetic'] = df['Diabetic'].replace(['No, borderline diabetes'], 'Yes')
df['Diabetic'] = df['Diabetic'].replace(['Yes (during pregnancy)'], 'No')
count_target = df['Diabetic'].value_counts()
count_target

count_target = df['GenHealth'].value_counts()
count_target

GenHealth = df.groupby(['GenHealth', 'HeartDisease']), ['HeartDisease'].count

sns.countplot(x='GenHealth', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('GenHealth and Heart Disease')

bins = [0, 18.5, 24.9, 29.9, 34.9, 39.9, float('inf')]
labels = [0, 1, 2, 3, 4, 5]
df['BMI_Category'] = pd.cut(df['BMI'], bins=bins, labels=labels, right=False)

df['BMI_Category'].value_counts()

df.drop('BMI', axis=1, inplace=True)

df.head()

df['BMI_Category'] = df['BMI_Category'].astype(int)

# Calculate maximum values
max_bmi = df['BMI_Category'].max()
max_physical_health = df['PhysicalHealth'].max()
max_mental_health = df['MentalHealth'].max()
max_genhealth = df['GenHealth'].max()
max_sleeptime = df['SleepTime'].max()


# Print the results
print("Maximum BMI:", max_bmi)
print("Maximum Physical Health:", max_physical_health)
print("Maximum Mental Health:", max_mental_health)
print("Maximum General Health:", max_genhealth)
print("Maximum Sleep Time:", max_sleeptime)

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
numerical_df = df.select_dtypes(include=['number'])

correlation_matrix = numerical_df.corr()

# Print the correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)

# Visualize the correlation matrix as a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', cbar=True)
plt.title('Correlation Matrix Heatmap')
plt.show()

df.describe()

"""**Outlier Removal**"""

import pandas as pd
from google.colab import files

numerical_columns = ['BMI_Category', 'PhysicalHealth', 'MentalHealth', 'SleepTime']


# Function to remove outliers using IQR
def remove_outliers_iqr(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.15)
        Q3 = df[col].quantile(0.85)
        IQR = Q3 - Q1  # Interquartile range
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        # Filter the DataFrame to exclude outliers
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

cleaned_data = remove_outliers_iqr(df, numerical_columns)

print(f"Original dataset size: {df.shape}")
print(f"Cleaned dataset size: {cleaned_data.shape}")

# Save the cleaned dataset to a CSV file
cleaned_data.to_csv('/content/drive/MyDrive/heart disease dataset/cleaned_Heart_disease.csv', index=False)
cleaned_data.head()

df = cleaned_data

df.describe()

"""LabelEncoded"""

from sklearn.preprocessing import LabelEncoder, MinMaxScaler

# Create a LabelEncoder object
le = LabelEncoder()

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Encode categorical features
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# Scale numerical features
numerical_cols = ['PhysicalHealth', 'MentalHealth', 'SleepTime']
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

df.head()

df.info()

import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = df.corr()

# Print the correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)

# Visualize the correlation matrix as a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', cbar=True)
plt.title('Correlation Matrix Heatmap')
plt.show()

"""# Analyzing for Heart Disease"""

count_target = df['HeartDisease'].value_counts()
count_target

labels = df['HeartDisease'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(df['HeartDisease'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Have Heart Disease')

from imblearn.over_sampling import SMOTE
# Split features and target
X = df.drop('HeartDisease', axis=1)
y = df['HeartDisease']

# Apply SMOTE to balance the dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine resampled data into a new DataFrame
balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['HeartDisease'])], axis=1)

# Print new class distribution
print("\nClass distribution after oversampling:")
print(balanced_data['HeartDisease'].value_counts())

# Save the balanced dataset to a CSV file
balanced_data.to_csv('balanced_Heart_disease.csv', index=False)

balanced_data.head()

balanced_data.info()

"""# MODEL

Splitting the Data into Train and Test Sets
"""

from sklearn.model_selection import train_test_split

# Assuming 'balanced_data' is your DataFrame
X = balanced_data.drop('HeartDisease', axis=1)
y = balanced_data['HeartDisease']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

from sklearn.impute import SimpleImputer

# Example of mean imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

"""# Accuracy"""

!pip install --upgrade scikit-learn lightgbm

!pip install --upgrade scikit-learn xgboost

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt

# KNN Model
print("K-Nearest Neighbors (KNN) Model\n")

knn = KNeighborsClassifier(n_neighbors=5)  # Adjust `n_neighbors` as needed
knn.fit(X_train, y_train)

# Predict on test data
y_pred_knn = knn.predict(X_test)

# Evaluation
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f"Accuracy: {accuracy_knn:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_knn))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_knn, cmap='Blues')
plt.title("KNN - Confusion Matrix")
plt.show()

from lightgbm import LGBMClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt

print("LightGBM Model\n")

model = LGBMClassifier(n_estimators=1000, learning_rate=0.05, max_depth=7)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)


accuracy_lgbm = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy_lgbm:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("LightGBM - Confusion Matrix")
plt.show()

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

imputer = SimpleImputer(strategy='mean')

X_train_imputed = imputer.fit_transform(X_train)

X_test_imputed = imputer.transform(X_test)

clf_DT = DecisionTreeClassifier().fit(X_train_imputed, y_train)

pred = clf_DT.predict(X_test_imputed)

print('Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, pred)))

print("Classification report:\n")
print(classification_report(y_test, pred))

print("Confusion Matrix:")
ConfusionMatrixDisplay.from_predictions(y_test, pred, cmap='YlOrRd')
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Random Forest
print("Random Forest Model\n")

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predict on test data
y_pred_rf = rf.predict(X_test)

# Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy: {accuracy_rf:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap='Oranges')
plt.title("Random Forest - Confusion Matrix")
plt.show()

from xgboost import XGBClassifier

# XGBoost
print("XGBoost Model\n")

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)

# Predict on test data
y_pred_xgb = xgb.predict(X_test)

# Evaluation
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy: {accuracy_xgb:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap='Purples')
plt.title("XGBoost - Confusion Matrix")
plt.show()

"""BEST MODEL"""

models = {
    "KNN": accuracy_knn,
    "LightGBM": accuracy_lgbm,
    "Decision Tree": accuracy_score(y_test, pred),
    "Random Forest": accuracy_rf,
    "XGBoost": accuracy_xgb
}

best_model = max(models, key=models.get)
best_accuracy = models[best_model]

print(f"\nThe best performing model is {best_model} with an accuracy of {best_accuracy:.4f}")

"""# Analyzing Stroke"""

count_target = df['Stroke'].value_counts()
count_target

stroke = df.groupby(['Stroke', 'HeartDisease']), ['HeartDisease'].count

sns.countplot(x='Stroke', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('Stroke and Heart Disease')

from imblearn.over_sampling import SMOTE
# Split features and target
X = df.drop('Stroke', axis=1)
y = df['Stroke']

# Apply SMOTE to balance the dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine resampled data into a new DataFrame
balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Stroke'])], axis=1)

# Print new class distribution
print("\nClass distribution after oversampling:")
print(balanced_data['Stroke'].value_counts())

# Save the balanced dataset to a CSV file
balanced_data.to_csv('balanced_stroke.csv', index=False)

balanced_data.head()

balanced_data.info()

"""# MODEL

Splitting the Data into Train and Test Sets
"""

from sklearn.model_selection import train_test_split

# Assuming 'balanced_data' is your DataFrame
X = balanced_data.drop('Stroke', axis=1)
y = balanced_data['Stroke']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt

# KNN Model
print("K-Nearest Neighbors (KNN) Model\n")

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Predict on test data
y_pred_knn = knn.predict(X_test)

# Evaluation
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f"Accuracy: {accuracy_knn:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_knn))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_knn, cmap='Blues')
plt.title("KNN - Confusion Matrix")
plt.show()

from lightgbm import LGBMClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt

# LightGBM Model
print("LightGBM Model\n")

lgbm = LGBMClassifier(random_state=42, n_estimators=100, learning_rate=0.1, max_depth=-1)
lgbm.fit(X_train, y_train)

# Predict on test data
y_pred_lgbm = lgbm.predict(X_test)

# Evaluation
accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)
print(f"Accuracy: {accuracy_lgbm:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_lgbm))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_lgbm, cmap='Blues')
plt.title("LightGBM - Confusion Matrix")
plt.show()

from sklearn.tree import DecisionTreeClassifier

# Decision Tree
print("Decision Tree Model\n")

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Predict on test data
y_pred_dt = dt.predict(X_test)

# Evaluation
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy: {accuracy_dt:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dt, cmap='Greens')
plt.title("Decision Tree - Confusion Matrix")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Random Forest
print("Random Forest Model\n")

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predict on test data
y_pred_rf = rf.predict(X_test)

# Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy: {accuracy_rf:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap='Oranges')
plt.title("Random Forest - Confusion Matrix")
plt.show()

from xgboost import XGBClassifier

# XGBoost
print("XGBoost Model\n")

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)

# Predict on test data
y_pred_xgb = xgb.predict(X_test)

# Evaluation
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy: {accuracy_xgb:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap='Purples')
plt.title("XGBoost - Confusion Matrix")
plt.show()

"""BEST MODEL"""

models = {
    "KNN": accuracy_knn,
    "LightGBM": accuracy_lgbm,
    "Decision Tree": accuracy_dt,
    "Random Forest": accuracy_rf,
    "XGBoost": accuracy_xgb
}

best_model = max(models, key=models.get)
best_accuracy = models[best_model]

print(f"\nThe best performing model for Stroke prediction is {best_model} with an accuracy of {best_accuracy:.4f}")

"""# Analyzing for Diabetic"""

count_target = df['Diabetic'].value_counts()
count_target

diabetic = df.groupby(['Diabetic', 'HeartDisease']), ['HeartDisease'].count

fig = plt.figure(figsize=(10,4))
sns.countplot(x='Diabetic', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('Diabetics and Heart Disease')

from imblearn.over_sampling import SMOTE
# Split features and target
X = df.drop('Diabetic', axis=1)
y = df['Diabetic']

# Apply SMOTE to balance the dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine resampled data into a new DataFrame
balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Diabetic'])], axis=1)

# Print new class distribution
print("\nClass distribution after oversampling:")
print(balanced_data['Diabetic'].value_counts())

# Save the balanced dataset to a CSV file
balanced_data.to_csv('balanced_diabetic.csv', index=False)

balanced_data.head()

balanced_data.info()

"""# MODEL"""

from sklearn.model_selection import train_test_split

X = balanced_data.drop('Diabetic', axis=1)
y = balanced_data['Diabetic']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% train, 20% test

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""CROSS VALIDATION"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
import xgboost as xgb
import lightgbm as lgb

# Random Forest Cross-Validation
print("\nCross-validating Random Forest...")
rf_model = RandomForestClassifier(random_state=42)
rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')
print(f"Random Forest Cross-Validation Accuracy: {np.mean(rf_cv_scores):.4f} (+/- {np.std(rf_cv_scores):.4f})")

# Decision Tree Cross-Validation
print("\nCross-validating Decision Tree...")
dt_model = DecisionTreeClassifier(random_state=42)
dt_cv_scores = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='accuracy')
print(f"Decision Tree Cross-Validation Accuracy: {np.mean(dt_cv_scores):.4f} (+/- {np.std(dt_cv_scores):.4f})")

# K-Nearest Neighbors (KNN) Cross-Validation
print("\nCross-validating KNN...")
knn_model = KNeighborsClassifier()
knn_cv_scores = cross_val_score(knn_model, X_train, y_train, cv=5, scoring='accuracy')
print(f"KNN Cross-Validation Accuracy: {np.mean(knn_cv_scores):.4f} (+/- {np.std(knn_cv_scores):.4f})")

# LightGBM Cross-Validation
print("\nCross-validating LightGBM...")
lgb_model = lgb.LGBMClassifier(random_state=42)
lgb_cv_scores = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring='accuracy')
print(f"LightGBM Cross-Validation Accuracy: {np.mean(lgb_cv_scores):.4f} (+/- {np.std(lgb_cv_scores):.4f})")

"""HYPERPARAMETER"""

# Random Forest with Grid Search
print("\nOptimizing Random Forest...")
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, scoring='accuracy')
rf_grid.fit(X_train, y_train)
print(f"Best Random Forest Parameters: {rf_grid.best_params_}")
print(f"Best CV Accuracy: {rf_grid.best_score_:.4f}")

# Decision Tree with Grid Search
print("\nOptimizing Decision Tree...")
dt_param_grid = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}
dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_param_grid, cv=5, scoring='accuracy')
dt_grid.fit(X_train, y_train)
print(f"Best Decision Tree Parameters: {dt_grid.best_params_}")
print(f"Best CV Accuracy: {dt_grid.best_score_:.4f}")

# K-Nearest Neighbors with Grid Search
print("\nOptimizing KNN...")
knn_param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance']
}
knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=5, scoring='accuracy')
knn_grid.fit(X_train, y_train)
print(f"Best KNN Parameters: {knn_grid.best_params_}")
print(f"Best CV Accuracy: {knn_grid.best_score_:.4f}")

from sklearn.model_selection import GridSearchCV
import xgboost as xgb

print("\nOptimizing XGBoost...")
xgb_param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 6, 10],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

xgb_grid = GridSearchCV(
    estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    param_grid=xgb_param_grid,
    cv=5,
    scoring='accuracy'
)

xgb_grid.fit(X_train, y_train)

print(f"Best XGBoost Parameters: {xgb_grid.best_params_}")
print(f"Best CV Accuracy: {xgb_grid.best_score_:.4f}")

# LightGBM with Grid Search
print("\nOptimizing LightGBM...")
lgb_param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [-1, 10, 20]
}
lgb_grid = GridSearchCV(lgb.LGBMClassifier(random_state=42), lgb_param_grid, cv=5, scoring='accuracy')
lgb_grid.fit(X_train, y_train)
print(f"Best LightGBM Parameters: {lgb_grid.best_params_}")
print(f"Best CV Accuracy: {lgb_grid.best_score_:.4f}")

"""MODEL WORK"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# Random Forest
print("Training Random Forest...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
rf_conf_matrix = confusion_matrix(y_test, rf_pred)
print("\nRandom Forest Results:")
print(f"Accuracy: {rf_accuracy:.4f}")
print("Confusion Matrix:")
print(rf_conf_matrix)

# Decision Tree
print("\nTraining Decision Tree...")
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
dt_conf_matrix = confusion_matrix(y_test, dt_pred)
print("\nDecision Tree Results:")
print(f"Accuracy: {dt_accuracy:.4f}")
print("Confusion Matrix:")
print(dt_conf_matrix)

# K-Nearest Neighbors (KNN)
print("\nTraining KNN...")
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_pred = knn_model.predict(X_test)
knn_accuracy = accuracy_score(y_test, knn_pred)
knn_conf_matrix = confusion_matrix(y_test, knn_pred)
print("\nKNN Results:")
print(f"Accuracy: {knn_accuracy:.4f}")
print("Confusion Matrix:")
print(knn_conf_matrix)

# LightGBM
print("\nTraining LightGBM...")
lgb_model = lgb.LGBMClassifier(random_state=42)
lgb_model.fit(X_train, y_train)
lgb_pred = lgb_model.predict(X_test)
lgb_accuracy = accuracy_score(y_test, lgb_pred)
lgb_conf_matrix = confusion_matrix(y_test, lgb_pred)
print("\nLightGBM Results:")
print(f"Accuracy: {lgb_accuracy:.4f}")
print("Confusion Matrix:")
print(lgb_conf_matrix)

"""MODEL"""

from lightgbm import LGBMClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt


print("LightGBM Model\n")

model = LGBMClassifier(n_estimators=1000, learning_rate=0.05, max_depth=7)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)


accuracy_lgbm = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy_lgbm:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("LightGBM - Confusion Matrix")
plt.show()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt

# KNN Model
print("K-Nearest Neighbors (KNN) Model\n")

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Predict on test data
y_pred_knn = knn.predict(X_test)

# Evaluation
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f"Accuracy: {accuracy_knn:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_knn))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_knn, cmap='Blues')
plt.title("KNN - Confusion Matrix")
plt.show()

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score # Import accuracy_score
import matplotlib.pyplot as plt

# Decision Tree
print("Decision Tree Model\n")

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Predict on test data
y_pred_dt = dt.predict(X_test)

# Evaluation
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy: {accuracy_dt:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dt, cmap='Greens')
plt.title("Decision Tree - Confusion Matrix")
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Random Forest
print("Random Forest Model\n")

# Initialize the Random Forest Classifier
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predict on test data
y_pred_rf = rf.predict(X_test)

# Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy: {accuracy_rf:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap='Purples')
plt.title("Random Forest - Confusion Matrix")
plt.show()

from xgboost import XGBClassifier

# XGBoost
print("XGBoost Model\n")

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)

# Predict on test data
y_pred_xgb = xgb.predict(X_test)

# Evaluation
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy: {accuracy_xgb:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap='Purples')
plt.title("XGBoost - Confusion Matrix")
plt.show()

"""BEST MODEL"""

models = {
    "KNN": accuracy_knn,
    "LightGBM": accuracy_lgbm,
    "Decision Tree": accuracy_dt,
    "Random Forest": accuracy_rf,
    "XGBoost": accuracy_xgb
}

best_model = max(models, key=models.get)
best_accuracy = models[best_model]

print(f"\nThe best performing model for Diabetic prediction is {best_model} with an accuracy of {best_accuracy:.4f}")

"""# Analyzing for Asthma"""

count_target = df['Asthma'].value_counts()
count_target

Asthma = df.groupby(['Asthma', 'HeartDisease']), ['HeartDisease'].count

sns.countplot(x='Asthma', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('Asthma and Heart Disease')

from imblearn.over_sampling import SMOTE
# Split features and target
X = df.drop('Asthma', axis=1)
y = df['Asthma']

# Apply SMOTE to balance the dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine resampled data into a new DataFrame
balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Asthma'])], axis=1)

# Print new class distribution
print("\nClass distribution after oversampling:")
print(balanced_data['Asthma'].value_counts())

# Save the balanced dataset to a CSV file
balanced_data.to_csv('balanced_asthma.csv', index=False)

balanced_data.head()

balanced_data.info()

"""# MODEL"""

from sklearn.model_selection import train_test_split

X = balanced_data.drop('Asthma', axis=1)
y = balanced_data['Asthma']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% train, 20% test

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

from sklearn.tree import DecisionTreeClassifier

# Decision Tree
print("Decision Tree Model\n")

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Predict on test data
y_pred_dt = dt.predict(X_test)

# Evaluation
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy: {accuracy_dt:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dt, cmap='Greens')
plt.title("Decision Tree - Confusion Matrix")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Random Forest
print("Random Forest Model\n")

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predict on test data
y_pred_rf = rf.predict(X_test)

# Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy: {accuracy_rf:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap='Oranges')
plt.title("Random Forest - Confusion Matrix")
plt.show()

from xgboost import XGBClassifier

# XGBoost
print("XGBoost Model\n")

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)

# Predict on test data
y_pred_xgb = xgb.predict(X_test)

# Evaluation
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy: {accuracy_xgb:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap='Purples')
plt.title("XGBoost - Confusion Matrix")
plt.show()

from lightgbm import LGBMClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt


print("LightGBM Model\n")

model = LGBMClassifier(n_estimators=1000, learning_rate=0.05, max_depth=7)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)


accuracy_lgbm = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy_lgbm:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("LightGBM - Confusion Matrix")
plt.show()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt

# KNN Model
print("K-Nearest Neighbors (KNN) Model\n")

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Predict on test data
y_pred_knn = knn.predict(X_test)

# Evaluation
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f"Accuracy: {accuracy_knn:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_knn))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_knn, cmap='Blues')
plt.title("KNN - Confusion Matrix")
plt.show()

"""BEST MODEL"""

models = {
    "Decision Tree": accuracy_dt,
    "Random Forest": accuracy_rf,
    "XGBoost": accuracy_xgb,
    "LightGBM": accuracy_lgbm,
    "KNN": accuracy_knn
}

best_model = max(models, key=models.get)
best_accuracy = models[best_model]

print(f"\nThe best performing model for Asthma prediction is {best_model} with an accuracy of {best_accuracy:.4f}")

"""# Analyzing for Kidney Disease"""

count_target = df['KidneyDisease'].value_counts()
count_target

KidneyDisease = df.groupby(['KidneyDisease', 'HeartDisease']), ['HeartDisease'].count

sns.countplot(x= 'KidneyDisease', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('KidneyDisease and Heart Disease')

from imblearn.over_sampling import SMOTE
# Split features and target
X = df.drop('KidneyDisease', axis=1)
y = df['KidneyDisease']

# Apply SMOTE to balance the dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine resampled data into a new DataFrame
balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['KidneyDisease'])], axis=1)

# Print new class distribution
print("\nClass distribution after oversampling:")
print(balanced_data['KidneyDisease'].value_counts())

# Save the balanced dataset to a CSV file
balanced_data.to_csv('balanced_kidney_disease.csv', index=False)

balanced_data.head()

balanced_data.info()

"""# MODEL"""

from sklearn.model_selection import train_test_split

X = balanced_data.drop('KidneyDisease', axis=1)
y = balanced_data['KidneyDisease']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% train, 20% test

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""# MODEL ACCURACY"""

from sklearn.tree import DecisionTreeClassifier

# Decision Tree
print("Decision Tree Model\n")

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Predict on test data
y_pred_dt = dt.predict(X_test)

# Evaluation
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy: {accuracy_dt:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dt, cmap='Greens')
plt.title("Decision Tree - Confusion Matrix")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Random Forest
print("Random Forest Model\n")

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predict on test data
y_pred_rf = rf.predict(X_test)

# Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy: {accuracy_rf:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap='Oranges')
plt.title("Random Forest - Confusion Matrix")
plt.show()

from xgboost import XGBClassifier

# XGBoost
print("XGBoost Model\n")

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)

# Predict on test data
y_pred_xgb = xgb.predict(X_test)

# Evaluation
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy: {accuracy_xgb:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap='Purples')
plt.title("XGBoost - Confusion Matrix")
plt.show()

from lightgbm import LGBMClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt


print("LightGBM Model\n")

model = LGBMClassifier(n_estimators=1000, learning_rate=0.05, max_depth=7)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)


accuracy_lgbm = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy_lgbm:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("LightGBM - Confusion Matrix")
plt.show()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt

# KNN Model
print("K-Nearest Neighbors (KNN) Model\n")

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Predict on test data
y_pred_knn = knn.predict(X_test)

# Evaluation
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f"Accuracy: {accuracy_knn:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_knn))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_knn, cmap='Blues')
plt.title("KNN - Confusion Matrix")
plt.show()

"""BEST ACCURACY"""

models = {
    "Logistic Regression": accuracy_lr,
    "Decision Tree": accuracy_dt,
    "Random Forest": accuracy_rf,
    "XGBoost": accuracy_xgb,
    "LightGBM": accuracy_lgbm,
    "KNN": accuracy_knn
}

best_model = max(models, key=models.get)
best_accuracy = models[best_model]

print(f"\nThe best performing model for Kidney Disease prediction is {best_model} with an accuracy of {best_accuracy:.4f}")

"""# Analyzing for Skin Cancer"""

count_target = df['SkinCancer'].value_counts()
count_target

SkinCancer = df.groupby(['SkinCancer', 'HeartDisease']), ['HeartDisease'].count

sns.countplot(x='SkinCancer', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('SkinCancer and Heart Disease')

from imblearn.over_sampling import SMOTE
# Split features and target
X = df.drop('SkinCancer', axis=1)
y = df['SkinCancer']

# Apply SMOTE to balance the dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine resampled data into a new DataFrame
balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['SkinCancer'])], axis=1)

# Print new class distribution
print("\nClass distribution after oversampling:")
print(balanced_data['SkinCancer'].value_counts())

# Save the balanced dataset to a CSV file
balanced_data.to_csv('balanced_skin_cancer.csv', index=False)

balanced_data.head()

balanced_data.info()

"""# MODEL

Splitting the Data into Train and Test Sets
"""

X = balanced_data.drop('SkinCancer', axis=1)
y = balanced_data['SkinCancer']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% train, 20% test

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

from sklearn.feature_selection import SelectFromModel

from sklearn.feature_selection import SelectFromModel
selector = SelectFromModel(RandomForestClassifier(n_estimators=100))
selector.fit(X_train, y_train)
X_train_selected = selector.transform(X_train)
X_test_selected = selector.transform(X_test)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy', verbose=2)
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)

X_test.reset_index(inplace=True, drop=True)
X_test

from sklearn.tree import DecisionTreeClassifier

# Decision Tree
print("Decision Tree Model\n")

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Predict on test data
y_pred_dt = dt.predict(X_test)

# Evaluation
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy: {accuracy_dt:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dt, cmap='Greens')
plt.title("Decision Tree - Confusion Matrix")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Random Forest
print("Random Forest Model\n")

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predict on test data
y_pred_rf = rf.predict(X_test)

# Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy: {accuracy_rf:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap='Oranges')
plt.title("Random Forest - Confusion Matrix")
plt.show()

from xgboost import XGBClassifier

# XGBoost
print("XGBoost Model\n")

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)

# Predict on test data
y_pred_xgb = xgb.predict(X_test)

# Evaluation
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy: {accuracy_xgb:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap='Purples')
plt.title("XGBoost - Confusion Matrix")
plt.show()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt

# KNN Model
print("K-Nearest Neighbors (KNN) Model\n")

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Predict on test data
y_pred_knn = knn.predict(X_test)

# Evaluation
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f"Accuracy: {accuracy_knn:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_knn))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_knn, cmap='Blues')
plt.title("KNN - Confusion Matrix")
plt.show()

from lightgbm import LGBMClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt


print("LightGBM Model\n")

model = LGBMClassifier(n_estimators=1000, learning_rate=0.05, max_depth=7)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)


accuracy_lgbm = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy_lgbm:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
plt.title("LightGBM - Confusion Matrix")
plt.show()

"""BEST ACCURACY"""

models = {
    "Decision Tree": accuracy_dt,
    "Random Forest": accuracy_rf,
    "XGBoost": accuracy_xgb,
    "LightGBM": accuracy_lgbm,
    "KNN": accuracy_knn
}

best_model = max(models, key=models.get)
best_accuracy = models[best_model]

print(f"\nThe best performing model for Skin Cancer prediction is {best_model} with an accuracy of {best_accuracy:.4f}")

"""# Analyzing for Others"""

count_target = df['Smoking'].value_counts()
count_target

smoking = df.groupby(['Smoking','HeartDisease'])['HeartDisease'].count()

sns.countplot(x='Smoking', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('Smokers and Heart Disease')

count_target = df['AlcoholDrinking'].value_counts()
count_target

alcohol = df.groupby(['AlcoholDrinking', 'HeartDisease']), ['HeartDisease'].count

sns.countplot(x='AlcoholDrinking', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('Heavy drinkers and Heart Disease')

count_target = df['PhysicalHealth'].value_counts()
count_target

import matplotlib.pyplot as plt
import seaborn as sns

# Grouping data (ensure the grouping is correct)
PhysicalHealth = df.groupby(['PhysicalHealth', 'HeartDisease'])['HeartDisease'].count()

# Setting up the figure size
plt.figure(figsize=(12, 8))  # Adjust the width and height as needed

# Creating the count plot
sns.countplot(x='PhysicalHealth', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

# Adding title and labels
plt.title('PhysicalHealth and Heart Disease', fontsize=16)  # Increase title size
plt.xlabel('Physical Health', fontsize=14)  # Increase x-axis label size
plt.ylabel('Count', fontsize=14)  # Increase y-axis label size

# Adjusting legend size
plt.legend(title='Heart Disease', fontsize=12, title_fontsize=14)

# Display the plot
plt.show()

count_target = df['DiffWalking'].value_counts()
count_target

walking = df.groupby(['DiffWalking', 'HeartDisease']), ['HeartDisease'].count

sns.countplot(x='DiffWalking', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('Difficulty Walking and Heart Disease')

count_target = df['Sex'].value_counts()
count_target

sex = df.groupby(['Sex', 'HeartDisease']), ['HeartDisease'].count

sns.countplot(x='Sex', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('Sex and Heart Disease')

count_target = df['PhysicalActivity'].value_counts()
count_target

PhysicalActivity = df.groupby(['PhysicalActivity', 'HeartDisease']), ['HeartDisease'].count

sns.countplot(x='PhysicalActivity', data=df, hue='HeartDisease', palette=["#9b5de5", "#fee440"])

plt.title('PhysicalActivity and Heart Disease')

"""Graph Describe"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
sns.violinplot(x='HeartDisease', y='PhysicalHealth', data=df, palette='Set2')
plt.title('Physical Health Distribution by Heart Disease Status')
plt.xlabel('Heart Disease')
plt.ylabel('Physical Health (Days of Poor Health)')


plt.subplot(1, 2, 2)
sns.violinplot(x='HeartDisease', y='MentalHealth', data=df, palette='Set3')
plt.title('Mental Health Distribution by Heart Disease Status')
plt.xlabel('Heart Disease')
plt.ylabel('Mental Health (Days of Poor Health)')


plt.tight_layout()
plt.show()

"""Correlation Matrix"""

import matplotlib.pyplot as plt
import seaborn as sns

# Function to create and display countplots
def create_countplot(x_col, hue_col, title, df):
    plt.figure(figsize=(8, 6))
    sns.countplot(x=x_col, data=df, hue=hue_col, palette=["#9b5de5", "#fee440"])
    plt.title(title)
    plt.show()

# Create countplots for various features
create_countplot('Smoking', 'HeartDisease', 'Smokers and Heart Disease', df)
create_countplot('AlcoholDrinking', 'HeartDisease', 'Heavy drinkers and Heart Disease', df)
create_countplot('DiffWalking', 'HeartDisease', 'Difficulty Walking and Heart Disease', df)
create_countplot('Sex', 'HeartDisease', 'Sex and Heart Disease', df)
create_countplot('PhysicalActivity', 'HeartDisease', 'Physical Activity and Heart Disease', df)

"""# RESULT ANALYSIS

# INTERFACE
"""

pip install --upgrade scikit-learn xgboost lightgbm

!pip install gradio

import pickle

disease_results = {
    "HeartDisease": {"best_accuracy": 0.9169, "model": "Random Forest"},
    "Stroke": {"best_accuracy": 0.9559, "model": "Random Forest"},
    "Asthma": {"best_accuracy": 0.8567, "model": "Random Forest"},
    "Diabetic": {"best_accuracy": 0.8644, "model": "Random Forest"},
    "KidneyDisease": {"best_accuracy": 0.9551, "model": "Random Forest"},
    "Skincancer": {"best_accuracy": 0.8864, "model": "Random Forest"},
}



# Save the results to a .pkl file
with open("best_accuracies.pkl", "wb") as file:
    pickle.dump(disease_results, file)

print("Best accuracies and models have been saved to best_accuracies.pkl")
print(f"\nThe best performing model is {best_model} for {best_disease} with an accuracy of {best_accuracy:.4f}")

import gradio as gr
import numpy as np
import pickle

# Load the trained Random Forest model
with open('/content/best_accuracies.pkl', 'rb') as file:
    random_forest_model = pickle.load(file)

def preprocess_inputs(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime):
    age_mapping = {"18-24": 0, "25-29": 1, "30-34": 2, "35-39": 3, "40-44": 4, "45-49": 5,
                   "50-54": 6, "55-59": 7, "60-64": 8, "65-69": 9, "70-74": 10, "75-79": 11, "80 or older": 12}
    race_mapping = {"White": 0, "Black": 1, "Asian": 2, "American Indian": 3, "Hispanic": 4, "Other": 5}
    gen_health_mapping = {"Excellent": 0, "Very Good": 1, "Good": 2, "Fair": 3, "Poor": 4}

    return np.array([[
        BMI,
        int(Smoking == "Yes"),
        PhysicalHealth,
        MentalHealth,
        int(DiffWalking == "Yes"),
        int(Sex == "Male"),
        age_mapping[AgeCategory],
        race_mapping[Race],
        int(PhysicalActivity == "Yes"),
        gen_health_mapping[GenHealth],
        SleepTime
    ]])

# Define prediction function
def predict_health_issues(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime):
    input_data = preprocess_inputs(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime)

    # Assuming the model is trained to predict multiple health issues
    rf_predictions = random_forest_model.predict(input_data)

    # Map predictions to health issues
    health_issues = ["Heart Disease", "Stroke", "Kidney Disease", "Asthma", "Diabetes", "Skin Cancer"]
    predictions_dict = {issue: "Yes" if pred == 1 else "No" for issue, pred in zip(health_issues, rf_predictions[0])}

    return predictions_dict

# Create Gradio interface
iface = gr.Interface(
    fn=predict_health_issues,
    inputs=[
        gr.Slider(10, 50, step=1, label="BMI"),
        gr.Radio(["Yes", "No"], label="Smoking"),
        gr.Slider(0, 30, step=1, label="Physical Health Issues (Days in Last Month)"),
        gr.Slider(0, 30, step=1, label="Mental Health Issues (Days in Last Month)"),
        gr.Radio(["Yes", "No"], label="Difficulty Walking"),
        gr.Radio(["Male", "Female"], label="Sex"),
        gr.Dropdown(["18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80 or older"], label="Age Category"),
        gr.Dropdown(["White", "Black", "Asian", "American Indian", "Hispanic", "Other"], label="Race"),
        gr.Radio(["Yes", "No"], label="Physical Activity"),
        gr.Dropdown(["Excellent", "Very Good", "Good", "Fair", "Poor"], label="General Health"),
        gr.Slider(0, 24, step=1, label="Sleep Time (Hours)")
    ],
    outputs=gr.JSON(label="Predicted Health Issues"),
    title="Health Issue Predictor",
    description="Input your information to predict potential health issues."
)

iface.launch()

import gradio as gr
import numpy as np
import pickle
from sklearn.ensemble import RandomForestClassifier

# Load the trained Random Forest model
with open('/content/best_accuracies.pkl', 'rb') as file:
    random_forest_model = pickle.load(file)

def preprocess_inputs(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime):
    age_mapping = {"18-24": 0, "25-29": 1, "30-34": 2, "35-39": 3, "40-44": 4, "45-49": 5,
                   "50-54": 6, "55-59": 7, "60-64": 8, "65-69": 9, "70-74": 10, "75-79": 11, "80 or older": 12}
    race_mapping = {"White": 0, "Black": 1, "Asian": 2, "American Indian": 3, "Hispanic": 4, "Other": 5}
    gen_health_mapping = {"Excellent": 0, "Very Good": 1, "Good": 2, "Fair": 3, "Poor": 4}

    return np.array([[
        BMI,
        int(Smoking == "Yes"),
        PhysicalHealth,
        MentalHealth,
        int(DiffWalking == "Yes"),
        int(Sex == "Male"),
        age_mapping[AgeCategory],
        race_mapping[Race],
        int(PhysicalActivity == "Yes"),
        gen_health_mapping[GenHealth],
        SleepTime
    ]])

# Define prediction function
def predict_health_issue(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime):
    input_data = preprocess_inputs(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime)

    # Get predictions for multiple health issues
    rf_predictions = random_forest_model.predict(input_data)
    conditions = ["Heart Disease", "Stroke", "Kidney Disease", "Asthma", "Diabetes", "Skin Cancer"]

    result = {condition: "Yes" if rf_predictions[i] == 1 else "No" for i, condition in enumerate(conditions)}
    return result

# Create Gradio interface
iface = gr.Interface(
    fn=predict_health_issue,
    inputs=[
        gr.Slider(10, 50, step=1, label="BMI"),
        gr.Radio(["Yes", "No"], label="Smoking"),
        gr.Slider(0, 30, step=1, label="Physical Health Issues (Days in Last Month)"),
        gr.Slider(0, 30, step=1, label="Mental Health Issues (Days in Last Month)"),
        gr.Radio(["Yes", "No"], label="Difficulty Walking"),
        gr.Radio(["Male", "Female"], label="Sex"),
        gr.Dropdown(["18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80 or older"], label="Age Category"),
        gr.Dropdown(["White", "Black", "Asian", "American Indian", "Hispanic", "Other"], label="Race"),
        gr.Radio(["Yes", "No"], label="Physical Activity"),
        gr.Dropdown(["Excellent", "Very Good", "Good", "Fair", "Poor"], label="General Health"),
        gr.Slider(0, 24, step=1, label="Sleep Time (Hours)")
    ],
    outputs=gr.JSON(),
    title="Health Issue Predictor",
    description="Input your information to predict potential health issues (Heart Disease, Stroke, Kidney Disease, Asthma, Diabetes, Skin Cancer)."
)

iface.launch()

import gradio as gr
import numpy as np
import pickle

# Load the trained Random Forest model
with open('/content/best_accuracies.pkl', 'rb') as file:
    random_forest_model = pickle.load(file)

def preprocess_inputs(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime):
    age_mapping = {"18-24": 0, "25-29": 1, "30-34": 2, "35-39": 3, "40-44": 4, "45-49": 5,
                   "50-54": 6, "55-59": 7, "60-64": 8, "65-69": 9, "70-74": 10, "75-79": 11, "80 or older": 12}
    race_mapping = {"White": 0, "Black": 1, "Asian": 2, "American Indian": 3, "Hispanic": 4, "Other": 5}
    gen_health_mapping = {"Excellent": 0, "Very Good": 1, "Good": 2, "Fair": 3, "Poor": 4}

    input_array = np.array([[
        BMI,
        int(Smoking == "Yes"),
        PhysicalHealth,
        MentalHealth,
        int(DiffWalking == "Yes"),
        int(Sex == "Male"),
        age_mapping[AgeCategory],
        race_mapping[Race],
        int(PhysicalActivity == "Yes"),
        gen_health_mapping[GenHealth],
        SleepTime
    ]])
    return input_array

def predict_health_conditions(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime):
    input_data = preprocess_inputs(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime)
    predictions = random_forest_model.predict(input_data)
    conditions = ["Heart Disease", "Stroke", "Asthma", "Diabetes", "Kidney Disease", "Skin Cancer"]
    result = {conditions[i]: "Yes" if predictions[i] == 1 else "No" for i in range(len(conditions))}
    return str(input_data), result

iface = gr.Interface(
    fn=predict_health_conditions,
    inputs=[
        gr.Slider(10, 50, step=1, label="BMI"),
        gr.Radio(["Yes", "No"], label="Smoking"),
        gr.Slider(0, 30, step=1, label="Physical Health Issues (Days in Last Month)"),
        gr.Slider(0, 30, step=1, label="Mental Health Issues (Days in Last Month)"),
        gr.Radio(["Yes", "No"], label="Difficulty Walking"),
        gr.Radio(["Male", "Female"], label="Sex"),
        gr.Dropdown(["18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80 or older"], label="Age Category"),
        gr.Dropdown(["White", "Black", "Asian", "American Indian", "Hispanic", "Other"], label="Race"),
        gr.Radio(["Yes", "No"], label="Physical Activity"),
        gr.Dropdown(["Excellent", "Very Good", "Good", "Fair", "Poor"], label="General Health"),
        gr.Slider(0, 24, step=1, label="Sleep Time (Hours)")
    ],
    outputs=[gr.Textbox(label="Preprocessed Input Array"), gr.JSON(label="Predicted Health Conditions")],
    title="Health Issue Predictor",
    description="Input your details and get predictions for potential health issues using a Random Forest model."
)

iface.launch()

!pip install ipywidgets

import gradio as gr
import numpy as np
import pickle

# Load the trained Random Forest model
with open("/content/best_accuracies.pkl", "rb") as model_file:
    model = pickle.load(model_file)

disease_labels = ["Heart Disease", "Stroke", "Asthma", "Diabetes", "Kidney Disease", "Skin Cancer"]

def predict_diseases(BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime):
    # Convert categorical inputs to numerical values
    Smoking = 1 if Smoking == "Yes" else 0
    DiffWalking = 1 if DiffWalking == "Yes" else 0
    Sex = 1 if Sex == "Male" else 0
    AgeCategory = ["18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80 or older"].index(AgeCategory)
    Race = ["White", "Black", "Asian", "American Indian", "Hispanic", "Other"].index(Race)
    PhysicalActivity = 1 if PhysicalActivity == "Yes" else 0
    GenHealth = ["Excellent", "Very Good", "Good", "Fair", "Poor"].index(GenHealth)

    # Create input array
    input_data = np.array([[BMI, Smoking, PhysicalHealth, MentalHealth, DiffWalking, Sex, AgeCategory, Race, PhysicalActivity, GenHealth, SleepTime]])

    # Predict diseases
    predictions = model.predict(input_data)

    # Convert predictions to dictionary
    result = {disease: "Yes" if pred == 1 else "No" for disease, pred in zip(disease_labels, predictions)}

    return result

iface = gr.Interface(
    fn=predict_diseases,
    inputs=[
        gr.Slider(10, 50, step=1, label="BMI"),
        gr.Radio(["Yes", "No"], label="Smoking"),
        gr.Slider(0, 30, step=1, label="Physical Health Issues (Days in Last Month)"),
        gr.Slider(0, 30, step=1, label="Mental Health Issues (Days in Last Month)"),
        gr.Radio(["Yes", "No"], label="Difficulty Walking"),
        gr.Radio(["Male", "Female"], label="Sex"),
        gr.Dropdown(["18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80 or older"], label="Age Category"),
        gr.Dropdown(["White", "Black", "Asian", "American Indian", "Hispanic", "Other"], label="Race"),
        gr.Radio(["Yes", "No"], label="Physical Activity"),
        gr.Dropdown(["Excellent", "Very Good", "Good", "Fair", "Poor"], label="General Health"),
        gr.Slider(0, 24, step=1, label="Sleep Time (Hours)")
    ],
    outputs=gr.JSON(),
    title="Multi-Disease Prediction",
    description="Enter your details to predict the risk of 6 diseases using a Random Forest model."
)

iface.launch()